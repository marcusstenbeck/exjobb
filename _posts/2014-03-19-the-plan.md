---
layout: post
title:  "The Plan"
date:   2014-03-19
categories: drafts
---

**Note:** *This is a DRAFT, and it shall not be considered serious in any way.*

## Preliminary Title

"Automatic Testing of Rendering in Mobile and Desktop Web Browsers"


## Problem Formulation

WebGL is a standard for drawing graphics in a web browser. It is a member of the web technologies commonly referred to as "HTML5", and since it's a web technology it faces the challenges unique to the web domain. There is an incredibly wide range of devices that could support any web standard, from TV sets to desktop workstations. Depending on how niche your target audience is of a particular web resource, a web developer is limited to using the technologies that are supported by the viewers device. It isn't currently widely understood how well WebGL works on devices. There are databases that have collected a large amount of information on the support of features across many devices. While this is a useful hint, it says little about how it looks when onscreen. Determining if an image looks correct to a human observer is a challenging, but interesting problem. Its solution is also useful; if a web developer could know more about the support for WebGL she could make a more informed decision whether to use it or not for a particular project, and if she decides to use it, also which technical limitations she will need to consider.

There is a lot of research that has been made regarding how to evaluate how good an image looks, and the topic is referred to as *Image Quality Assessment*. The topic is widely researched in context of evaluating compression algorithms. Many methods rely on having a reference image that is considered "perfect", which is compared to an image with some type of distortion. This *Image Quality Assessment* could prove useful in determining if two WebGL-renders are the same.


*Question:* Does WebGL look "right" on *this* device?

*Strategy to answer question:* Run an automatic test suite and get an answer.


## How to do this?

Set up a test case for each thing we want to test. This is a challenge in itself – what is useful to test? *Maybe there's some literature on this...*

When at least one test case has been defined then do, for each test case

1. Collect data (screenshots) from device.
2. Examine features in the data samples that describe the data sample. This is the Image Quality Assessment part. We will benefit from knowing as much as possible about the input data (the screenshots). This will determine which methods are suitable.
3. Based on the features data from the previous step, try to classify the data. This part can either be trivial, like using only one of the feature metrics like Mean Squared Error. It can also be more advanced, involving some form of machine learning.
4. Manually put labels on data classes, e.g. "correct", "error-1", etc. This step should be pretty simple.


## Scope

This thesis project shall focus on the image assessment and classification of data based on device screenshots. The desired outcome is a foundation for building an automatic visual testing framework that is agnostic in regard to the device being tested.


## Expected Required Simplifications

The **development of test cases** will be such that interesting and useful aspects are tested, but this thesis project is in no means intended to result in an exhaustive test suite.

**Data collection** is not interesting in the context of the thesis work, so this step will be performed in such a way that enough data is available. However, automation or optimization of this step is outside of the scope of this thesis project.


## Rough Time Plan

* En tidplan för examensarbetets genomförande inklusive planerade datum för halvtidskontroll
och framläggning

ONE *MILLION* weeks. Stuff takes time, son. *<–– See? Draft. Told ya. This is a joke.*

| Week  | Work                                       |
| ----- | ------------------------------------------ |
| 1-2   | Problem formulation and planning           |
| 3-7   | ... work ...                               |
| 8     | Half-time check                            |
| 9-14  | ... work ...                               |
| 15-20 | Report writing                             |

...


## Literature

* Planerad litteraturbas

[Using Image Quality Assessment to Test Rendering Algorithms][amann et. al] by Amann et. al is the paper that originally turned me on to the thesis topic. In the paper they use *Image Quality Assessment* and *Machine Learning* to create a program that may work when testing procedural rendering algorithms.

[Alan Bovik][bovik] has many books and papers on image processing. [The Essential Guide to Image Processing][bovik-1] contains good information on *Image Quality Assessment*. Further exploration of his works and collaborations reveal many more resources on the topic, especially the very useful article [Mean Squared Error: Love it or leave it? A new look at Signal Fidelity Measures][zhou-1].

If classification needs to be more advanced, machine learning could be applied. This is expected to be needed. The online course on [Machine Learning at Coursera][coursera-ml] by [Andrew Ng][ng], Stanford, contains many links to machine learning textbooks and academic papers.



[amann et. al]: http://wscg.zcu.cz/wscg2013/program/full/E43-full.pdf
[ng]: http://scholar.google.se/citations?user=JgDKULMAAAAJ
[bovik]: http://scholar.google.se/citations?user=p-PC50wAAAAJ&hl=sv&oi=ao
[bovik-1]: http://www.amazon.com/Essential-Guide-Image-Processing/dp/0123744571/
[zhou-1]: http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=4775883
[perceptual-digital-imaging]: http://books.google.se/books?id=ciOF1H-wZacC&pg=PA81#v=onepage&q&f=false
[coursera-ml]: https://www.coursera.org/course/ml